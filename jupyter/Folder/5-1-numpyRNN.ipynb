{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x-np.max(x))\n",
    "#     e_x = np.exp(x)\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def initialize_adam(parameters):\n",
    "    L = len(parameters)/2\n",
    "    v = {}\n",
    "    s = {}\n",
    "    for l in range(L):\n",
    "        v['dW'+str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)\n",
    "        v['db'+str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)\n",
    "        s['dW'+str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)\n",
    "        s['db'+str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)\n",
    "    return v,s\n",
    "\n",
    "def update_parameters_with_adam(parameters,grads,v,s,t,\n",
    "                                learning_rate=0.01,beta1=0.9,beta2=0.999,epsilon=1e-8):\n",
    "    # parameters['W'+str(l)] = W1\n",
    "    # grads['dW'+str(l)] = dW1\n",
    "    L = len(parameters)//2\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v['dW'+str(l+1)] = beta1*v['dW'+str(l+1)]+(1-beta1)*grads['dW'+str(l+1)]\n",
    "        v['db'+str(l+1)] = beta1*v['db'+str(l+1)]+(1-beta1)*grads['db'+str(l+1)]\n",
    "        s['dW'+str(l+1)] = beta2*s['dW'+str(l+1)]+(1-beta2)*(grads['dW'+str(l+1)]**2)\n",
    "        s['db'+str(l+1)] = beta2*s['db'+str(l+1)]+(1-beta2)*(grads['db'+str(l+1)]**2)\n",
    "        \n",
    "        v_corrected['dW'+str(l+1)] = v['dW'+str(l+1)]/(1-beta1**t)\n",
    "        v_corrected['db'+str(l+1)] = v['db'+str(l+1)]/(1-beta1**t)\n",
    "        s_corrected['dW'+str(l+1)] = s['dW'+str(l+1)]/(1-beta2**t)\n",
    "        s_corrected['db'+str(l+1)] = s['db'+str(l+1)]/(1-beta2**t)\n",
    "        \n",
    "        parameters['W'+str(l+1)] = parameters['W'+str(l+1)]-learning_rate*v_corrected['dW'+str(l+1)]/np.sqrt(s['dW'+str(l+1)]+epsilon)\n",
    "        parameters['b'+str(l+1)] = parameters['b'+str(l+1)]-learning_rate*v_corrected['db'+str(l+1)]/np.sqrt(s['db'+str(l+1)]+epsilon)\n",
    "    return parameters, v, s\n",
    "\n",
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    Wax = parameters['Wax']\n",
    "    Waa = parameters['Waa']\n",
    "    ba = parameters['ba']\n",
    "    by = parameters['by']\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Waa,a_prev) + np.dot(Waa, xt) + ba)\n",
    "    yt_pred = softmax(np.dot(Way, a_next) + by)\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    return a_next, yt_pred, cache\n",
    "\n",
    "def rnn_forward(x, a0, parameters):\n",
    "    caches = []\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters['Wya'].shape\n",
    "    \n",
    "    a = np.zeros((n_a,m,T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    a_next = a0\n",
    "    \n",
    "    for t in range(T_x):\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t],a_next,parameters)\n",
    "        a[:,:,t] = a_next\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        caches.append(cache)\n",
    "    caches = (caches, x)\n",
    "    return a, y_pred, caches\n",
    "\n",
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    # weights matrix of the forget gate\n",
    "    Wf = parameters['Wf']\n",
    "    bf = parameters['bf']\n",
    "    # weights matrix og the update gate\n",
    "    Wi = parameters['Wi']\n",
    "    bi = parameters['bi']\n",
    "    # weights matrix of the first tanh\n",
    "    Wc = parameters['Wc']\n",
    "    bc = parameters['bc']\n",
    "    # weights matrix of the output gate\n",
    "    Wo = parameters['Wo']\n",
    "    bo = parameters['bo']\n",
    "    # weights matrix relating the hidden-state to the output\n",
    "    Wy = parameters['Wy']\n",
    "    by = parameters['by']\n",
    "    \n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "    \n",
    "    concat = np.zeros((n_x+n_a, m))\n",
    "    concat[:n_a, :] = a_prev\n",
    "    concat[n_a:, :] = xt\n",
    "    \n",
    "    ft = sigmoid(np.dot(Wf, concat)+bf)\n",
    "    it = sigmoid(np.dot(Wi, concat)+bi)\n",
    "    cct = np.tanh(np.dot(Wc, concat)+bc)\n",
    "    c_next = ft*c_prev + it*cct\n",
    "    ot = sigmoid(np.dot(Wo, concat)+bo)\n",
    "    a_next = ot*np.tanh(c_next)\n",
    "    \n",
    "    yt_pred = softmax(np.dot(Wy, a_next)+by)\n",
    "    caches = (a_next,c_next,a_prev,c_prev,ft,it,cct,ot,xt,parameters)\n",
    "    return a_next, c_next, yt_pred, cache\n",
    "\n",
    "deff lstm_forward(x, a0, parameters):\n",
    "    caches = []\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters['Wy'].shape\n",
    "    \n",
    "    a = np.zeros((n_a, m, T_X))\n",
    "    c = np.zeros((n_a, m, T_x))\n",
    "    y = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    a_next = a0\n",
    "    c_next = np.zeros((n_a, m))\n",
    "    \n",
    "    for t in range(T_x):\n",
    "        a_next,c_next,yt, cache = lstm_cell_forward(x[:,:,t],a_next,c_next,parameters)\n",
    "        a[:,:,t] = a_next\n",
    "        y[:,:,t] = yt\n",
    "        c[:,:,t] = c_next\n",
    "        cachess.append(cache)\n",
    "    caches = (caches, x)\n",
    "    return a,y,c,caches\n",
    "\n",
    "def rnn_cell_backward(da_next, cache):\n",
    "    (a_next,a_prev,xt,parameters) = cache\n",
    "    Wax = parameters['Wax']\n",
    "    Waa = parameters['Waa']\n",
    "    Wya = parameters['Wya']\n",
    "    ba = parameters['ba']\n",
    "    by = parameters['by']\n",
    "    \n",
    "    dtanh = (1-a_next**2)*da_next\n",
    "    dxt = np.dot(Wax.T, dtanh)\n",
    "    dWax = np.dot(dtanh, xt.T)\n",
    "    \n",
    "    da_prev = np.dot(Waa.T, dtanh)\n",
    "    dWaa = np.dot(dtanh, a_prev.T)\n",
    "    \n",
    "    dba = np.sum(dtanh, keepdims=True, axis=-1)\n",
    "    \n",
    "    gradient = {\"dxt\":dxt,\"da_prev\":da_prev,\"dWax\":dWax,\"dWaa\":dWaa, \"dba\":dba}\n",
    "    return gradient\n",
    "\n",
    "def rnn_backward(da, caches):\n",
    "    (caches, x) = caches\n",
    "    (a1,a0,x1,parameters) = caches[0]\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    \n",
    "    dx = np.zeros((n_x, m, T_x))\n",
    "    dWax = np.zeros((n_a,n_x))\n",
    "    dWaa = np.zeros((n_a, n_a))\n",
    "    dba = np.zeros((n_a, 1))\n",
    "    da0 = np.zeros((n_a, m))\n",
    "    da_prevt = np.zeros((n_a, m))\n",
    "    \n",
    "    for t in reversed(range(T_x)):\n",
    "        gradients = rnn_cell_backward(da[:,:,t]+da_prevt, caches[t])\n",
    "        dxt, da_prevt,dWaxt,dWaat,dbat = gradients['dxt'],gradients['da_prev'],gradients['dWax'],gradients['dWaa'],gradients['dba']\n",
    "        dx[:,:,t] = dxt\n",
    "        dWax += dWaxt\n",
    "        dWaa += dWaat\n",
    "        dba += dbat\n",
    "        \n",
    "    da0 = da_prevt\n",
    "    gradients = {'dx':dx,\"da0\":da0,\"dWax\":dWax,\"dWaa\":dWaa,\"dba\":dba}\n",
    "    return gradients\n",
    "\n",
    "def lstm_cell_backward(da_next, dc_next, cache):\n",
    "    (a_next,c_next,a_prev,c_prev,ft,it,cct,ot,xt,parameters) = cache\n",
    "    n_x,m = xt.shape\n",
    "    n_a,m = a_next.shape\n",
    "    \n",
    "    dot = da_next * np.tanh(c_next)*ot*(1-ot)\n",
    "    dcct = (dc_next*it+ot*(1-np.square(np.tanh(c_next)))*it*da_next)*(1-np.squarere(cct))\n",
    "    dit = (dc_next*cct+ot*(1-np.square(np.tanh(c_nextxt)))*cct*da_next)*it*(1-it)\n",
    "    dft = (dc_next*c_prev+ot*(1-np.square(np.tanh(c_next)))*c_prev*da_next)*ft*(1-ft)\n",
    "    \n",
    "    dWf = np.dot(dft,np.concatenate((a_prev,xt),axis=0).T)\n",
    "    dWi = np.dot(dit,np.concatenate((a_prev,xt),axis=0).T)\n",
    "    dWc = np.dott(dcct,np.concatenate((a_prev,xt),axis=0).T)\n",
    "    dWo = np.dot(dot,np.concatenate((a_prev,xt),axis=0).T)\n",
    "    dbf = np.sum(dft,axis=1,keepdims=True)\n",
    "    dbi = np.sum(dit, aixs=1,keepdims=True)\n",
    "    dbc = np.sum(dcct,axis=1,keepdims=True)\n",
    "    dbo = np.sum(dot,axis=1,keepdims=True)\n",
    "    \n",
    "    da_prev = np.dot(parameter['Wf'][:,:n_a].T,dft)+np.dot(parameters['Wi'][:,:n_a].T,dit)+np.dot(\n",
    "                    parameters['Wc'][:,:n_a].T,dcct)+np.dot(parameters['Wo'][:,:n_a].T,dot)\n",
    "    dc_prev = dc_next*ft+ot*(1-np.square(np.tanh(c_next)))*ft*da_next\n",
    "    dxt = np.dot(parameters['Wf'][:,n_a:].T,dft)+np.dot(parameters['Wi'][:,n_a:].T,dit)+np.dot(\n",
    "                    parameters['Wc'][:,n_a:].T,dcct)+np.dot(parameters['Wo'][:,n_a:].T,dot)\n",
    "    gradients = {'dxt':dxt,\"da_prev\":da_prev,\"dc_prev\":dc_prev,\"dWf\":dWf,\"dbf\":dbf,\"dWi\":dWi,\n",
    "                \"dbi\":dbi,\"dWc\":dWc,\"dbc\":dbc,\"dWo\":dWo,\"dbo\":dbo}\n",
    "    return gradients\n",
    "\n",
    "def lstm_backward(da, caches):\n",
    "    (caches,x) = caches\n",
    "    (a1,c1,a0,c0,f1,i1,cc1,o1,x1,parameters) = caches[0]\n",
    "    \n",
    "    n_a,m,T_x = da.shape\n",
    "    n_x,m = x1.shape\n",
    "    \n",
    "    dx = np.zeros((n_x,m,T_x))\n",
    "    da0 = np.zeros((n_a,m))\n",
    "    da_prevt = np.zeros((n_a,m))\n",
    "    dc_prevt = np.zeros((n_a,m))\n",
    "    dWf = np.zeros((n_a,n_a+n_x))\n",
    "    dWi = np.zeros((n_a,n_a+n_x))\n",
    "    dWc = np.zeros((n_a,n_a+n_x))\n",
    "    dWo = np.zeros((n_a,n_a+n_x))\n",
    "    dbf = np.zeros((n_a,1))\n",
    "    dbi = np.zeros((n_a,1))\n",
    "    dbc = np.zeros((n_a,1))\n",
    "    dbo = np.zeros((n_a,1))\n",
    "    \n",
    "    for t in reversed(range(T_x)):\n",
    "        gradients = lstm_cell_backward(da[:,:,t]+da_prevt,dc_prevt,caches[t])\n",
    "        dx[:,:,t] = gradients['dxt']\n",
    "        dWf += gradients['dWf']\n",
    "        dWi += gradients['dWi']\n",
    "        dWc += gradients['dWc']\n",
    "        dWo += gradients['dWo']\n",
    "        dbf += gradients['dbf']\n",
    "        dbi += gradients['dbi']\n",
    "        dbc += gradients['dbc']\n",
    "        dbo += gradients['dbo']\n",
    "    da0 = gradients['da_prev']\n",
    "    gradients = {'dx':dx,\"da0\":da0,\"dWf\":dWf,\"dbf\":dbf,\"dWi\":dWi,\"dbi\":dbi,\n",
    "                \"dWc\":dWc,\"dbc\":dbc,\"dWo\":dWo,\"dbo\":dbo}\n",
    "    return gradients\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
