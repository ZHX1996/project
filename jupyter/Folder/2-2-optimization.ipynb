{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import math \n",
    "import matplotlib.pyplot as %pagelt\n",
    "\n",
    "def load_params_and_grads(seed=1):\n",
    "    np.random.seed(seed)\n",
    "    W1 = np.random.randn(2,3)\n",
    "    b1 = np.random.randn(2,1)\n",
    "    W2 = np.random.randn(3,3)\n",
    "    b1 = np.random.randn(3,1)\n",
    "    \n",
    "    dW1 = np.random.randn(2,3)\n",
    "    db1 = np.random.randn(2,1)\n",
    "    dW2 = np.random.randn(3,3)\n",
    "    db2 = np.random.randn(3,1)\n",
    "    return W1,b1,W2,b2,dW1,db1,dW2,db2\n",
    "\n",
    "def relu(X):\n",
    "    return np.maximum(0,X)\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1./(1+np.exp(-X))\n",
    "\n",
    "def init_params(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    params={}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1,L):\n",
    "        params[\"W\"+str(l)]=np.random.randn(layer_dims[l],layer_dims[l-1])*np.sqrt(2/layer_dims[l-1])\n",
    "        params['b'+str(l)]=np.random.randn(layer_dims[l],1)\n",
    "    return params\n",
    "\n",
    "def forward_propagation(X,params):\n",
    "    W1 = params[\"W1\"]\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "    W3 = params['W3']\n",
    "    b3 = params['b3']\n",
    "    \n",
    "    z1 = np.dot(W1,X)+b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(W2,a1)+b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(W3,a2)+b3\n",
    "    a3 = sigmoid(z3)\n",
    "    cache = (z1,a1,W1,b1,z2,a2,W2,b2,z3,a3,W3,b3)\n",
    "    return a3, cache\n",
    "\n",
    "def backward(X,Y,cache):\n",
    "    m = X.shape[1]\n",
    "    (z1,a1,W1,b1,z2,a2,W2,b2,z3,a3,W3,b3)=cache\n",
    "    dz3 = 1./m*(a3-Y)\n",
    "    dW3 = np.dot(dz3,a2.T)\n",
    "    db3 = np.sum(dz3,axis=1,keepdims=True)\n",
    "    \n",
    "    da2 = 1./m*np.dot(W3.T,dz3)\n",
    "    dz2 = np.multiply(da2,np.int64(a2>0))\n",
    "    dW2 = np.dot(dz2,a1.T)\n",
    "    db2 = np.sum(dz2,axis=1,keepdims=True)\n",
    "    \n",
    "    da1 = 1./m*np.dot(W2.T,dz2)\n",
    "    dz1 = np.multiply(da1,np.int64(a1>0))\n",
    "    dW1 = np.dot(dz1,X.T)\n",
    "    db1 = np.sum(dz1,axis=1,keepdims=True)\n",
    "    \n",
    "    gradients = {\"dz3\":dz3,\"dW3\":dW3,\"db3\":db3,\n",
    "                \"da2\":da2,\"dz2\":dz2,\"dW2\":dW2,\"db2\":db2,\n",
    "                \"da1\":da1,\"dz1\":dz1,\"dW1\":dW1,\"db1\":db1}\n",
    "    return gradients\n",
    "\n",
    "def compute_cost(a3,Y):\n",
    "    m = Y.shape[1]\n",
    "    logprobs = np.multiply(-np.log(a3),Y)+np.multiply(-np.log(1-a3),1-Y)\n",
    "    cost = 1./m*np.sum(logprobs)\n",
    "    return cost\n",
    "\n",
    "def predict(X,y,params):\n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m),dtype=np.int)\n",
    "    a3,cache = forward_propagation(X,params)\n",
    "    for i in range(a3.shape[1]):\n",
    "        if a3[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    return p\n",
    "\n",
    "def predict_dec(params, X):\n",
    "    a3, cache = forward_propagation(X,params)\n",
    "    predictions = (a3>0.5)\n",
    "    return predictions\n",
    "\n",
    "def plot_decision_boundary(model,X,y):\n",
    "    x_min,x_max = X[0,:].min(),X[0,:].max()\n",
    "    y_min,y_max = X[1,:].min(),X[1,:].max()\n",
    "    \n",
    "    h = 0.01\n",
    "    xx,yy = np.meshgrid(np.range(x_min,x_max),np.range(y_min,y_max))\n",
    "    Z = model(np.c_[xx.ravel(),yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx,yy,Z,cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0,:],X[1,:],c=y,cmap=plt.cm.Spectral)\n",
    "    plt.show()\n",
    "    \n",
    "def load_dataset():\n",
    "    np.random.seed(3)\n",
    "    train_X,train_Y = sklearn.datasets.make_moons(n_samples=300,noise=.2)\n",
    "    plt.scatter(train_X[:,0],train_X[:,1],c=train_Y,s=40,cmap=plt.cm.Spectral)\n",
    "    plt.show()\n",
    "    train_X = train_X.T\n",
    "    train_Y = train_Y.reshape((1,train_Y.shape[0]))\n",
    "    return train_X,train_Y\n",
    "\n",
    "def update_params_with_gd(params,grads,learning_rate):\n",
    "    L = len(params)//2\n",
    "    for l in range(L):\n",
    "        params[\"W\"+str(l+1)] = params['W'+str(l)]-learning_rate*grads['W'+str(l+1)]\n",
    "        params[\"b\"+str(l+1)] = params['b'+str(l)]-learning_rate*grads['b'+str(l+1)]\n",
    "    return params\n",
    "\n",
    "def random_mini_batches(X,Y,mini_batch_size=64,seed=0):\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]\n",
    "    min_batches = []\n",
    "    \n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:,permutation]\n",
    "    shuffled_Y = Y[:,permutation].reshape((1,m))\n",
    "    \n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size)\n",
    "    for k in range(num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:,k*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:,k*mini_batch_size]\n",
    "        mini_batch = (mini_batch_X,mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "    if m%mini_batch_size!=0:\n",
    "        mini_batch_X = shuffled_X[:,num_complete_minibatches*mini_batch_size:m]\n",
    "        mini_batch_Y = shuffled_Y[:,num_complete_minibatches*mini_batch_size:m]\n",
    "        mini_batch = (mini_batch_X,mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches\n",
    "\n",
    "# momentum\n",
    "def init_velocity(params):\n",
    "    L = len(params)//2\n",
    "    v = {}\n",
    "    for l in range(L):\n",
    "        v[\"dW\"+str(l+1)]=np.zeros(params[\"W\"+str(l+1)].shape)\n",
    "        v[\"db\"+str(l+1)]=np.zeros(params[\"b\"+str(l+1)].shape)\n",
    "    return v\n",
    "\n",
    "def update_params_with_momentum(params,grads,v,beta,learning_rate):\n",
    "    L = len(params)//2\n",
    "    for l in range(L):\n",
    "        v['dW'+str(l+1)] = beta*v['dW'+str(l+1)]+(1-beta)*grads['dW'+str(l+1)]\n",
    "        v['db'+str(l+1)] = beta*v['db'+str(l+1)]+(1-beta)*grads['db'+str(l+1)]\n",
    "        params['W'+str(l+1)]=params['W'+str(l+1)]-learning_rate*v['dW'+str(l+1)]\n",
    "        params['b'+str(l+1)]=params['b'+str(l+1)]-learning_rate*v['db'+str(l+1)]\n",
    "    return params,v\n",
    "\n",
    "def init_adam(params):\n",
    "    L = len(params)//2\n",
    "    v = {}\n",
    "    s = {}\n",
    "\n",
    "    for l in range(L):\n",
    "        v['dW'+str(l+1)]=np.zeros(params['W'+str(l+1)].shape)\n",
    "        v['db'+str(l+1)]=np.zeros(params['b'+str(l+1)].shape)\n",
    "        s['dW'+str(l+1)]=np.zeros(params['W'+str(l+1)].shape)\n",
    "        s['db'+str(l+1)]=np.zeros(params['b'+str(l+1)].shape)\n",
    "    return v,s\n",
    "\n",
    "def update_params_with_adam(params,grads,v,s,t,learning_rate=0.01,beta1=0.9,beta2=0.999,epsilon=1e-8):\n",
    "    L = len(params)//2\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "\n",
    "    for l in range(L):\n",
    "        v['dW'+str(l+1)]=beta1*v['dW'+str(l+1)]+(1-beta1)*grads['dW'+str(l+1)]\n",
    "        v['db'+str(l+1)]=beta1*v['db'+str(l+1)]+(1-beta1)*grads['db'+str(l+1)]\n",
    "\n",
    "        v_corrected['dW'+str(l+1)]=v['dW'+str(l+1)]/(1-beta1**t)\n",
    "        v_corrected['db'+str(l+1)]=v['db'+str(l+1)]/(1-beta1**t)\n",
    "\n",
    "        s['dW'+str(l+1)]=beta2*s['dW'+str(l+1)]+(1-beta2)*grads['dW'+str(l+1)]**2\n",
    "        s['db'+str(l+1)]=beta2*s['db'+str(l+1)]+(1-beta2)*grads['db'+str(l+1)]**2\n",
    "\n",
    "        s_corrected['dW'+str(l+1)]=s['dW'+str(l+1)]/(1-beta2**t)\n",
    "        s_corrected['db'+str(l+1)]=s['db'+str(l+1)]/(1-beta2**t)\n",
    "\n",
    "        params['W'+str(l+1)]=params['W'+str(l+1)]-learning_rate*(v_corrected['dW'+str(l+1)]/np.sqrt(s_corrected['dW'+str(l+1)]+epsilon))\n",
    "        params['b'+str(l+1)]=params['b'+str(l+1)]-learning_rate*(v_corrected['db'+str(l+1)]/np.sqrt(s_corrected['db'+str(l+1)]+epsilon))\n",
    "    return params,v,s\n",
    "\n",
    "def model(X,Y,layers_dims,optimizer,learning_rate=0.0007,mini_batch_size=64,beta=0.9,beta1=0.9,beta2=0.999,epsilon=1e-8,num_epochs=10000,print_cost=True):\n",
    "    L = len(layers_dims)\n",
    "    costs=[]\n",
    "    t=0\n",
    "    seed=10\n",
    "\n",
    "    params = init_params(layers_dims)\n",
    "    if optimizer=='gd':\n",
    "        pass\n",
    "    elif optimizer=='momentum':\n",
    "        v = init_velocity(params)\n",
    "    elif optimizer=='adam':\n",
    "        v,s = init_adam(params)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        seed += 1\n",
    "        minibatches = random_mini_batches(X,Y,mini_batch_size,seed)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "            (minibatch_X,minibatch_Y) = minibatch\n",
    "            a3,caches = forward_propagation(minibatch_X, params)\n",
    "            cost = compute_cost(a3,minibatch_Y)\n",
    "            grads = backward_propagation(minibatch_X,minibatch_Y,caches)\n",
    "\n",
    "            if optimizer=='gd':\n",
    "                params = update_params_with_gd(params,grads,learning_rate)\n",
    "            elif optimizer=='momentum':\n",
    "                params,v = update_params_with_momentum(params,grads,v,beta,learning_rate)\n",
    "            elif optimizer=='adam':\n",
    "                t += 1\n",
    "                params,v,s = update_params_with_adam(params,grads,v,s,t,learning_rate,beta1,beta2,epsilon)\n",
    "\n",
    "        \n",
    "        if print_cost and i%1000==0:\n",
    "            print(\"cost after epoch %i: %f\" % (i, cost))\n",
    "        if print_cost and i%100==0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel(\"epochs(per 100)\")\n",
    "    plt.title(\"learning rate= \" + str(learning_rate))\n",
    "    plt.show()\n",
    "    return params\n",
    "\n",
    "if __name__=='__main__':\n",
    "    train_X, train_Y = load_dataset()\n",
    "    layers_dims = [train_X.shape[0],5,2,1]\n",
    "    # gradient descent\n",
    "    # params = model(train_X, train_Y, layers_dims, optimizer='gd')\n",
    "    # plt.title('Model with gradient descent optimization')\n",
    "\n",
    "    # momentum\n",
    "    # params = model(train_X, train_Y, layers_dims, beta=0.9, optimizer='momentum')\n",
    "    # plt.title('Model with Momentum optimization')\n",
    "\n",
    "    # adam\n",
    "    params = model(train_X,train_Y,layers_dims,optimizer='adam')\n",
    "    plt.title('Model with Adam optimization')\n",
    "\n",
    "    predictions = predict(train_X, train_Y, params)\n",
    "    axes = plt.gca()\n",
    "    axes.set_xlim([-1.5,2.5])\n",
    "    axes.set_ylim([-1,1.5])\n",
    "    plot_decision_boundary(lambda x: predict_dec(params, x.T), train_X, train_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
