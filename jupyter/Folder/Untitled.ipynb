{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file1 = pd.read_excel(\"C:/Users/Administrator.SC-201905252025/Desktop/terror.xlsx\", sheet_name='网络搜索情况', names=['手机号','搜索内容', '搜索引擎', '时间'])\n",
    "df1 = pd.DataFrame(file1)\n",
    "file2 = pd.read_excel(\"C:/Users/Administrator.SC-201905252025/Desktop/terror.xlsx\", sheet_name='VPN使用情况', names=['手机号'])\n",
    "df2 = pd.DataFrame(file2)\n",
    "file3 = pd.read_excel(\"C:/Users/Administrator.SC-201905252025/Desktop/terror.xlsx\", sheet_name='敏感聊天软件使用情况', names=['手机号'])\n",
    "df3 = pd.DataFrame(file3)\n",
    "file4 = pd.read_excel(\"C:/Users/Administrator.SC-201905252025/Desktop/terror.xlsx\",\n",
    "                      sheet_name='网络发言情况', names=[\"应用\",\"发送账号\",\"接收账号\",\"内容\",\"用户名\",\"手机号\",\"群号\",\"行为\",\"时间\",\"sfz\"])\n",
    "df4 = pd.DataFrame(file4)\n",
    "\n",
    "tmp1 = df1.groupby(['手机号'])['手机号'].count()\n",
    "tmp1 = pd.DataFrame({'网络搜索次数':tmp1})\n",
    "tmp1.reset_index(inplace=True)\n",
    "\n",
    "tmp2 = df2.groupby(['手机号'])['手机号'].count()\n",
    "tmp2 = pd.DataFrame({'VPN使用次数':tmp2})\n",
    "tmp2.reset_index(inplace=True)\n",
    "\n",
    "tmp3 = df3.groupby(['手机号'])['手机号'].count()\n",
    "tmp3 = pd.DataFrame({'敏感聊天软件使用次数':tmp3})\n",
    "tmp3.reset_index(inplace=True)\n",
    "\n",
    "tmp4 = df4.groupby(['手机号'])['手机号'].count()\n",
    "tmp4 = pd.DataFrame({'网络发言次数':tmp4})\n",
    "tmp4.reset_index(inplace=True)\n",
    "\n",
    "union1 = pd.merge(tmp1,tmp2,how='outer',on='手机号')\n",
    "union2 = pd.merge(tmp3,tmp4,how='outer',on='手机号')\n",
    "df = pd.merge(union1,union2,how='outer',on='手机号')\n",
    "\n",
    "writer = pd.ExcelWriter(\"C:/Users/Administrator.SC-201905252025/Desktop/terrorunion.xlsx\", engine='xlsxwriter',options={'strings_to_urls':False})\n",
    "writer.book.strings_to_urls = False\n",
    "df.to_excel(writer, 'Sheet1', index='False')\n",
    "writer.save()\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<meta name=\"description\" content=\"在自然语言处理的一些任务中，我们总是要先将源数据处理成符合模型算法输入的形式，在经过分词、停用词移除等其他操作之后，我们一般需要将文本转化成矩阵的形式，这一步也被称为特征提取（feature extraction）或者向量化（vectorization）。本文简要介绍一下词袋模式(BoW)，tf-idf，Hash，lsa, lda, Doc2vec这几种方法的概念和使用方式。\"><meta name=\"keywords\" content=\"NLP, Python\"><meta property=\"og:type\" content=\"article\"><meta property=\"og:title\" content=\"NLP中的文本预处理——向量化\"><meta property=\"og:url\" content=\"http://www.nlpuser.com/nlp/2018/11/08/Vectorization-in-text/\"><meta property=\"og:site_name\" content=\"waker\"><meta property=\"og:description\" content=\"在自然语言处理的一些任务中，我们总是要先将源数据处理成符合模型算法输入的形式，在经过分词、停用词移除等其他操作之后，我们一般需要将文本转化成矩阵的形式，这一步也被称为特征提取（feature extraction）或者向量化（vectorization）。本文简要介绍一下词袋模式(BoW)，tf-idf，Hash，lsa, lda, Doc2vec这几种方法的概念和使用方式。\"><meta property=\"og:locale\" content=\"en\"><meta property=\"og:image\" content=\"http://cloudpicture.oss-cn-hangzhou.aliyuncs.com/18-11-6/66557193.jpg\"><meta name=\"twitter:card\" content=\"summary\"><meta name=\"twitter:title\" content=\"NLP中的文本预处理——向量化\"><meta name=\"twitter:description\" content=\"在自然语言处理的一些任务中，我们总是要先将源数据处理成符合模型算法输入的形式，在经过分词、停用词移除等其他操作之后，我们一般需要将文本转化成矩阵的形式，这一步也被称为特征提取（feature extraction）或者向量化（vectorization）。本文简要介绍一下词袋模式(BoW)，tf-idf，Hash，lsa, lda, Doc2vec这几种方法的概念和使用方式。\"><meta name=\"twitter:image\" content=\"http://cloudpicture.oss-cn-hangzhou.aliyuncs.com/18-11-6/66557193.jpg\"><meta name=\"twitter:title\" content=\"NLP中的文本预处理——向量化\">\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "str1 = \"\"\"\n",
    "<meta name=\"description\" content=\"在自然语言处理的一些任务中，我们总是要先将源数据处理成符合模型算法输入的形式，在经过分词、停用词移除等其他操作之后，我们一般需要将文本转化成矩阵的形式，这一步也被称为特征提取（feature extraction）或者向量化（vectorization）。本文简要介绍一下词袋模式(BoW)，tf-idf，Hash，lsa, lda, Doc2vec这几种方法的概念和使用方式。\">\n",
    "<meta name=\"keywords\" content=\"NLP, Python\">\n",
    "<meta property=\"og:type\" content=\"article\">\n",
    "<script type=\"text/javascript\" id=\"hexo.configurations\">\n",
    "\n",
    "    algolia: {\n",
    "      applicationID: '',\n",
    "      apiKey: '',\n",
    "      indexName: '',\n",
    "      hits: {\"per_page\":10},\n",
    "      labels: {\"input_placeholder\":\"Search for Posts\",\"hits_empty\":\"We didn't find any results for the search: ${query}\",\"hits_stats\":\"${hits} results found in ${time} ms\"}\n",
    "    }\n",
    "  };\n",
    "</script>\n",
    "<meta property=\"og:title\" content=\"NLP中的文本预处理——向量化\">\n",
    "<meta property=\"og:url\" content=\"http://www.nlpuser.com/nlp/2018/11/08/Vectorization-in-text/\">\n",
    "<meta property=\"og:site_name\" content=\"waker\">\n",
    "<meta property=\"og:description\" content=\"在自然语言处理的一些任务中，我们总是要先将源数据处理成符合模型算法输入的形式，在经过分词、停用词移除等其他操作之后，我们一般需要将文本转化成矩阵的形式，这一步也被称为特征提取（feature extraction）或者向量化（vectorization）。本文简要介绍一下词袋模式(BoW)，tf-idf，Hash，lsa, lda, Doc2vec这几种方法的概念和使用方式。\">\n",
    "<meta property=\"og:locale\" content=\"en\">\n",
    "<script type=\"text/javascript\" id=\"hexo.configurations\">\n",
    "  var NexT = window.NexT || {};\n",
    "  var CONFIG = {\n",
    "    root: '',\n",
    "    scheme: 'Mist',\n",
    "    sidebar: {\"position\":\"left\",\"display\":\"post\",\"offset\":12,\"offset_float\":0,\"b2t\":false,\"scrollpercent\":false},\n",
    "    fancybox: true,\n",
    "    motion: true,\n",
    "    duoshuo: {\n",
    "      userId: '0',\n",
    "      author: 'Author'\n",
    "    };\n",
    "</script>\n",
    "\n",
    "<scip type=\"text/javascript\" id=\"hexo.configurations\">\n",
    "  var NexT = window.NexT || {};\n",
    "  var CONFIG = {\n",
    "    root: '',\n",
    "    scheme: 'Mist',\n",
    "    sidebar: {\"position\":\"left\",\"display\":\"post\",\"offset\":12,\"offset_float\":0,\"b2t\":false,\"scrollpercent\":false},\n",
    "    fancybox: true,\n",
    "    motion: true,\n",
    "    duoshuo: {\n",
    "      userId: '0',\n",
    "      author: 'Author'\n",
    "    };\n",
    "</scip>\n",
    "<meta property=\"og:image\" content=\"http://cloudpicture.oss-cn-hangzhou.aliyuncs.com/18-11-6/66557193.jpg\">\n",
    "<meta name=\"twitter:card\" content=\"summary\">\n",
    "<meta name=\"twitter:title\" content=\"NLP中的文本预处理——向量化\">\n",
    "<meta name=\"twitter:description\" content=\"在自然语言处理的一些任务中，我们总是要先将源数据处理成符合模型算法输入的形式，在经过分词、停用词移除等其他操作之后，我们一般需要将文本转化成矩阵的形式，这一步也被称为特征提取（feature extraction）或者向量化（vectorization）。本文简要介绍一下词袋模式(BoW)，tf-idf，Hash，lsa, lda, Doc2vec这几种方法的概念和使用方式。\">\n",
    "<meta name=\"twitter:image\" content=\"http://cloudpicture.oss-cn-hangzhou.aliyuncs.com/18-11-6/66557193.jpg\">\n",
    "<script type=\"text/javascript\" id=\"hexo.configurations\">\n",
    "  var NexT = window.NexT || {};\n",
    "  var CONFIG = {\n",
    "    root: '',\n",
    "    scheme: 'Mist',\n",
    "    sidebar: {\"position\":\"left\",\"display\":\"post\",\"offset\":12,\"offset_float\":0,\"b2t\":false,\"scrollpercent\":false},\n",
    "    fancybox: true,\n",
    "    motion: true,\n",
    "    duoshuo: {\n",
    "      userId: '0',\n",
    "      author: 'Author'\n",
    "    },\n",
    "    algolia: {\n",
    "      applicationID: '',\n",
    "      apiKey: '',\n",
    "      indexName: '',\n",
    "      hits: {\"per_page\":10},\n",
    "      labels: {\"input_placeholder\":\"Search for Posts\",\"hits_empty\":\"We didn't find any results for the search: ${query}\",\"hits_stats\":\"${hits} results found in ${time} ms\"}\n",
    "    }\n",
    "  };\n",
    "</script>\n",
    "<meta name=\"twitter:title\" content=\"NLP中的文本预处理——向量化\">\n",
    "\"\"\"\n",
    "\n",
    "# str1 = str1.replace('<script', '</script>')\n",
    "#     pattern = re.compile(r'(l)(.*)(l)')\n",
    "#   如果想包括两个l，则用pattern.sub(r\\1''\\3,Content)\n",
    "#     return pattern.sub(r'',content)\n",
    "# pattern = re.compile()\n",
    "str1 = re.sub(r'\\n', '', str1)\n",
    "pattern = re.compile(r'<script.*?</script>')\n",
    "str1 = pattern.sub(r'', str1)\n",
    "pattern = re.compile(r'<scip.*?</scip>')\n",
    "result = pattern.sub(r'', str1)\n",
    "# result = re.sub(r'.*?', '', str1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python35\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# file1 = pd.read_excel(\"C:/Users/Administrator.SC-201905252025/Desktop/ctrlvpn.xlsx\", sheet_name='1', names=['account'])\n",
    "# df1 = pd.DataFrame(file1)\n",
    "\n",
    "file2 = pd.read_excel(\"C:/Users/Administrator.SC-201905252025/Desktop/hacker.xlsx\", dtype={'authaccount':object},\n",
    "                      sheet_name='Sheet1', names=['authaccount','from_id','to_id','username','groupnum','sfz'])\n",
    "df2 = pd.DataFrame(file2)\n",
    "\n",
    "# df2 = df2['authaccount']\n",
    "# df = pd.merge(df1,df2, left_on='account',right_on='authaccount', how='inner')\n",
    "# print(df)\n",
    "\n",
    "tmp1 = df2[['groupnum','from_id']]\n",
    "tmp2 = df2[['groupnum', 'to_id']]\n",
    "tmp2 = tmp2.rename(columns={'to_id':'from_id'})\n",
    "tmp3 = pd.concat([tmp1,tmp2],axis=0)\n",
    "tmp3.drop_duplicates(['groupnum','from_id'], keep='first', inplace=True)\n",
    "tmp4 = tmp3.groupby(['groupnum'])['from_id'].count()\n",
    "tmp5 = pd.DataFrame({'fromto':tmp4})\n",
    "tmp5.reset_index(inplace=True)\n",
    "\n",
    "tmp1 = df2[['groupnum', 'authaccount']]\n",
    "tmp1.drop_duplicates(['groupnum','authaccount'],keep='first',inplace=True)\n",
    "tmp2 = tmp1.groupby(['groupnum'])['authaccount'].count()\n",
    "tmp3 = pd.DataFrame({'auth':tmp2})\n",
    "tmp3.reset_index(inplace=True)\n",
    "\n",
    "df = pd.merge(tmp3,tmp5,how='outer',on='groupnum')\n",
    "writer = pd.ExcelWriter(\"C:/Users/Administrator.SC-201905252025/Desktop/hackergroup.xlsx\", engine='xlsxwriter',options={'strings_to_urls':False})\n",
    "writer.book.strings_to_urls = False\n",
    "df.to_excel(writer, 'Sheet1', index='False')\n",
    "writer.save()\n",
    "print(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
