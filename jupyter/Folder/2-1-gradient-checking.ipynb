{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.39243295089e-08 good\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(X):\n",
    "    return np.maximum(0,X)\n",
    "\n",
    "def sigmod(X):\n",
    "    return 1./(1+np.exp(-X))\n",
    "\n",
    "def dictionary_to_vector(params):\n",
    "    keys = []\n",
    "    count = 0\n",
    "    for key in ['W1','b1','W2','b2','W3','b3']:\n",
    "        new_vector = np.reshape(params[key],(-1,1))\n",
    "        keys = keys + [key]*new_vector.shape[0]\n",
    "\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta,new_vector), axis=0)\n",
    "        count += 1\n",
    "    return theta,keys\n",
    "\n",
    "def vector_to_dictionary(theta):\n",
    "    params = {}\n",
    "    params[\"W1\"] = theta[:20].reshape((5,4))\n",
    "    params[\"b1\"] = theta[20:25].reshape((5,1))\n",
    "    params[\"W2\"] = theta[25:40].reshape((3,5))\n",
    "    params[\"b2\"] = theta[40:43].reshape((3,1))\n",
    "    params[\"W3\"] = theta[43:46].reshape((1,3))\n",
    "    params[\"b3\"] = theta[46:47].reshape((1,1))\n",
    "    return params\n",
    "\n",
    "def gradients_to_vector(gradients):\n",
    "    count = 0\n",
    "    for key in [\"dW1\",\"db1\", \"dW2\",\"db2\",\"dW3\",\"db3\"]:\n",
    "        new_vector = np.reshape(gradients[key],(-1,1))\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta,new_vector),axis = 0)\n",
    "        count += 1\n",
    "    return theta\n",
    "\n",
    "def forward_propagation(x,theta):\n",
    "    J = thetha*x\n",
    "    return J\n",
    "\n",
    "def backward_propagation(x, theta):\n",
    "    dtheta = x\n",
    "    return dtheta\n",
    "\n",
    "def gradient_check(x, theta, epsilon=1e-7):\n",
    "    thetaplus = theta + epsilon\n",
    "    thetaminus = theta - epsilon\n",
    "    J_plus = forward_propagation(x,thetaplus)\n",
    "    J_minus = forward_propagation(x, thetaminus)\n",
    "    gradapprox = (J_plus-J_minus)/(2*epsilon)\n",
    "\n",
    "    grad = backward_propagation(x, theta)\n",
    "\n",
    "    numerator = np.linalg.norm(grad-gradapprox)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "\n",
    "    difference = numerator/denominator\n",
    "\n",
    "    if difference < epsilon:\n",
    "        print(\"the gradient is correct\")\n",
    "    else:\n",
    "        print(\"the gradient is wrong\")\n",
    "    return difference\n",
    "\n",
    "def forward_propagation_n(X,Y,params):\n",
    "    m = X.shape[1]\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "    W3 = params['W3']\n",
    "    b3 = params['b3']\n",
    "    \n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3,A2)+b3\n",
    "    A3 = sigmod(Z3)\n",
    "    \n",
    "    logprobs = np.multiply(-np.log(A3),Y)+np.multiply(-np.log(1-A3),1-Y)\n",
    "    cost = 1./m*np.sum(logprobs)\n",
    "    cache = (Z1,A1,W1,b1,Z2,A2,W2,b2,Z3,A3,W3,b3)\n",
    "    return cost, cache\n",
    "\n",
    "def backward_propagation_n(X,Y,cache):\n",
    "    m = X.shape[1]\n",
    "    (Z1,A1,W1,b1,Z2,A2,W2,b2,Z3,A3,W3,b3) = cache\n",
    "    dZ3 = A3-Y\n",
    "    dW3 = 1./m*np.dot(dZ3,A2.T)\n",
    "    db3 = 1./m*np.sum(dZ3,axis=1,keepdims=True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T,dZ3)\n",
    "    dZ2 = np.multiply(dA2,np.int64(A2>0))\n",
    "    dW2 = 1./m*np.dot(dZ2,A1.T)\n",
    "    db2 = 1./m*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T,dZ2)\n",
    "    dZ1 = np.multiply(dA1,np.int64(A1>0))\n",
    "    dW1 = 1./m*np.dot(dZ1,X.T)\n",
    "    db1 = 1./m*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ3\":dZ3,\"dW3\":dW3,\"db3\":db3,\n",
    "                \"dA2\":dA2,\"dZ2\":dZ2,\"dW2\":dW2,\"db2\":db2,\n",
    "                \"dA1\":dA1,\"dZ1\":dZ1,\"dW1\":dW1,\"db1\":db1}\n",
    "    return gradients\n",
    "\n",
    "def gradient_check_n(params,gradients,X,Y,epsilon=1e-7):\n",
    "    params_values,_=dictionary_to_vector(params)\n",
    "    grads = gradients_to_vector(gradients)\n",
    "    num_params = params_values.shape[0]\n",
    "    J_plus = np.zeros((num_params, 1))\n",
    "    J_minus = np.zeros((num_params, 1))\n",
    "    gradapprox = np.zeros((num_params,1))\n",
    "    \n",
    "    for i in range(num_params):\n",
    "        thetaplus = np.copy(params_values)\n",
    "        thetaplus[i][0] += epsilon\n",
    "        J_plus[i],_ = forward_propagation_n(X,Y,vector_to_dictionary(thetaplus))\n",
    "        \n",
    "        thetaminus = np.copy(params_values)\n",
    "        thetaminus[i][0] -= epsilon\n",
    "        J_minus[i],_ = forward_propagation_n(X,Y,vector_to_dictionary(thetaminus))\n",
    "        gradapprox[i]=(J_plus[i]-J_minus[i])/(2.*epsilon)\n",
    "    \n",
    "    numerator = np.linalg.norm(grads-gradapprox)\n",
    "    denominator = np.linalg.norm(grads)+np.linalg.norm(gradapprox)\n",
    "    difference = numerator/denominator\n",
    "    if difference>1e-7:\n",
    "        print(str(difference) + \" something wrong\")\n",
    "    else:\n",
    "        print(str(difference) + \" good\")\n",
    "    return difference\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1)\n",
    "    X = np.random.randn(4,3)\n",
    "    Y = np.array([1, 1, 0])\n",
    "    W1 = np.random.randn(5,4) \n",
    "    b1 = np.random.randn(5,1) \n",
    "    W2 = np.random.randn(3,5) \n",
    "    b2 = np.random.randn(3,1) \n",
    "    W3 = np.random.randn(1,3) \n",
    "    b3 = np.random.randn(1,1) \n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "\n",
    "    cost,cache = forward_propagation_n(X,Y,params)\n",
    "    gradients = backward_propagation_n(X,Y,cache)\n",
    "    difference = gradient_check_n(params,gradients,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
